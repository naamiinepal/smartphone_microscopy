{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/SSD0/safal/AI_assisted_microscopy_system/codes/.venv/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torchvision.ops import box_iou\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"/mnt/Enterprise/safal/AI_assisted_microscopy_system/\"\n",
    "sample_types = [\"smartphone_sample\", \"smartphone_reference\", \"brightfield_sample\", \"brightfield_reference\"]\n",
    "model_type = \"retinanet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision_recall_f1(pred_annotations_file, gt_annotations_file):\n",
    "    # Precision x Recall is obtained individually by each class\n",
    "    # Loop through each class and calculate the precision and recall\n",
    "\n",
    "    # Precision = TP / (TP + FP)\n",
    "    # Recall = TP / (TP + FN)\n",
    "    gt_annotations = json.load(open(gt_annotations_file))\n",
    "    pred_annotations = json.load(open(pred_annotations_file))\n",
    "\n",
    "    gt_annotations_df = pd.DataFrame(gt_annotations[\"annotations\"])\n",
    "    pred_annotations_df = pd.DataFrame(pred_annotations)\n",
    "\n",
    "    # change bbox width and height to x2, y2\n",
    "    pred_annotations_df[\"bbox\"] = pred_annotations_df[\"bbox\"].apply(\n",
    "        lambda x: [x[0], x[1], x[0] + x[2], x[1] + x[3]]\n",
    "    )\n",
    "    gt_annotations_df[\"bbox\"] = gt_annotations_df[\"bbox\"].apply(\n",
    "        lambda x: [x[0], x[1], x[0] + x[2], x[1] + x[3]]\n",
    "    )\n",
    "\n",
    "    categories = sorted(gt_annotations_df.category_id.unique())\n",
    "\n",
    "    # dataframe to store the precision, recall and f1 score for each class\n",
    "    metrics_df = pd.DataFrame(\n",
    "        columns=[\"category\", \"precision\", \"recall\", \"f1_score\", \"TP\", \"FP\"]\n",
    "    )\n",
    "\n",
    "    for category in categories:\n",
    "        # get the ground truth annotations for the current class\n",
    "        gt_annotations_df_class = gt_annotations_df[\n",
    "            gt_annotations_df.category_id == category\n",
    "        ]\n",
    "        # get the predicted annotations for the current class\n",
    "        pred_annotations_df_class = pred_annotations_df[\n",
    "            pred_annotations_df.category_id == category\n",
    "        ]\n",
    "\n",
    "        # sort the predicted annotations by score\n",
    "        pred_annotations_df_class = pred_annotations_df_class.sort_values(\n",
    "            by=\"score\", ascending=False\n",
    "        )\n",
    "\n",
    "        # filter predictions with score > 0.3\n",
    "        pred_annotations_df_class = pred_annotations_df_class[\n",
    "            pred_annotations_df_class.score > 0.1\n",
    "        ]\n",
    "\n",
    "        true_positives_class = 0\n",
    "        false_positives_class = 0\n",
    "\n",
    "        # get image ids for the current class from both ground truth and predicted annotations\n",
    "        image_ids = pred_annotations_df_class[\"image_id\"].unique()\n",
    "        images_len = len(image_ids)\n",
    "\n",
    "        for image in image_ids:\n",
    "            # get the ground truth annotations for the current image\n",
    "            gt_annotations_df_image = gt_annotations_df_class[\n",
    "                gt_annotations_df_class.image_id == image\n",
    "            ]\n",
    "            # get the predicted annotations for the current image\n",
    "            pred_annotations_df_image = pred_annotations_df_class[\n",
    "                pred_annotations_df_class.image_id == image\n",
    "            ]\n",
    "\n",
    "            # get the ground truth bounding boxes\n",
    "            gt_bboxes = list(gt_annotations_df_image.bbox.values)\n",
    "            gt_bboxes = torch.tensor(gt_bboxes)\n",
    "\n",
    "            # get the predicted bounding boxes\n",
    "            pred_bboxes = list(pred_annotations_df_image.bbox.values)\n",
    "            pred_bboxes = torch.tensor(pred_bboxes)\n",
    "\n",
    "            if len(gt_bboxes) == 0:\n",
    "                false_positives_class += len(pred_bboxes)\n",
    "                continue\n",
    "\n",
    "            # get the intersection over union for each predicted bounding box\n",
    "            ious = box_iou(pred_bboxes, gt_bboxes)\n",
    "\n",
    "            # get the maximum iou for each ground truth bounding box\n",
    "            max_ious, _ = torch.max(ious, dim=0)\n",
    "\n",
    "            # get the indices of the predicted bounding boxes with iou > 0.5\n",
    "            tp_indices = torch.where(max_ious >= 0.5)[0]\n",
    "            # print(ious)\n",
    "\n",
    "            # get the indices of the predicted bounding boxes with iou < 0.5\n",
    "            fp_indices = torch.where(max_ious < 0.5)[0]\n",
    "\n",
    "            # update the true positives and false positives\n",
    "            true_positives_class += len(tp_indices)\n",
    "            false_positives_class += len(fp_indices)\n",
    "\n",
    "        # calculate the precision and recall\n",
    "        precision = true_positives_class / (\n",
    "            true_positives_class + false_positives_class\n",
    "        )\n",
    "        recall = true_positives_class / gt_annotations_df_class.shape[0]\n",
    "\n",
    "        category_name = gt_annotations[\"categories\"][category][\"name\"]\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "        category_metrics_df = pd.DataFrame(\n",
    "            {\n",
    "                \"category\": category_name,\n",
    "                \"precision\": precision,\n",
    "                \"recall\": recall,\n",
    "                \"f1_score\": f1_score,\n",
    "                \"TP\": true_positives_class,\n",
    "                \"FP\": false_positives_class,\n",
    "            },\n",
    "            index=[0],\n",
    "        )\n",
    "\n",
    "        # concatenate the metrics for the current class to the metrics dataframe\n",
    "        metrics_df = pd.concat([metrics_df, category_metrics_df], axis=0)\n",
    "\n",
    "    return metrics_df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine metrics from all folds into a single csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_type:  smartphone_sample\n",
      "  category  precision_mean  precision_std  recall_mean  recall_std  \\\n",
      "0   Crypto        0.422358       0.054276     0.822667    0.024773   \n",
      "1  Giardia        0.291082       0.030711     0.862016    0.057513   \n",
      "\n",
      "   f1_score_mean  f1_score_std  \n",
      "0       0.556436      0.047688  \n",
      "1       0.433837      0.033559  \n",
      "sample_type:  smartphone_reference\n",
      "  category  precision_mean  precision_std  recall_mean  recall_std  \\\n",
      "0   Crypto        0.504157       0.051878     0.784562    0.045485   \n",
      "1  Giardia        0.641107       0.030574     0.915260    0.016448   \n",
      "\n",
      "   f1_score_mean  f1_score_std  \n",
      "0       0.613215      0.050537  \n",
      "1       0.753749      0.023947  \n",
      "sample_type:  brightfield_sample\n",
      "  category  precision_mean  precision_std  recall_mean  recall_std  \\\n",
      "0   Crypto        0.479775       0.045590     0.945138    0.028571   \n",
      "1  Giardia        0.499527       0.032596     0.946294    0.020586   \n",
      "\n",
      "   f1_score_mean  f1_score_std  \n",
      "0       0.635760      0.044433  \n",
      "1       0.653364      0.028777  \n",
      "sample_type:  brightfield_reference\n",
      "  category  precision_mean  precision_std  recall_mean  recall_std  \\\n",
      "0   Crypto        0.600674       0.121902     0.966356    0.017384   \n",
      "1  Giardia        0.905362       0.020524     0.984545    0.004398   \n",
      "\n",
      "   f1_score_mean  f1_score_std  \n",
      "0       0.734561      0.088251  \n",
      "1       0.943187      0.010864  \n"
     ]
    }
   ],
   "source": [
    "# combine metrics from all folds into single csv with mean and variance\n",
    "for sample_type in sample_types:\n",
    "    metrics_df_all = None\n",
    "    for fold in range(1, 6):\n",
    "        gt_annotation_file = os.path.join(\n",
    "            BASE_DIR,\n",
    "            f\"cysts_dataset_all/{sample_type}/fold_{fold}/{sample_type}_coco_annos_val.json\",\n",
    "        )\n",
    "\n",
    "        pred_annotation_file = os.path.join(\n",
    "            BASE_DIR,\n",
    "            f\"outputs/{sample_type}/{model_type}/fold_{fold}/results.bbox.json\"\n",
    "        )\n",
    "        if not os.path.exists(pred_annotation_file):\n",
    "            continue\n",
    "\n",
    "        metrics_file = os.path.join(\n",
    "            BASE_DIR,\n",
    "            f\"outputs/{sample_type}/{model_type}/fold_{fold}/metrics_pr.csv\"\n",
    "        )\n",
    "        metrics_df = calculate_precision_recall_f1(pred_annotation_file, gt_annotation_file)\n",
    "        \n",
    "        if fold == 1:\n",
    "            metrics_df_all = metrics_df\n",
    "        else:\n",
    "            metrics_df_all = pd.concat([metrics_df_all, metrics_df], ignore_index=True)\n",
    "    \n",
    "    # calculate class wise mean and standard deviation\n",
    "    metrics_df_all = metrics_df_all.groupby(\"category\").agg(\n",
    "        {\n",
    "            \"precision\": [\"mean\", \"std\"],\n",
    "            \"recall\": [\"mean\", \"std\"],\n",
    "            \"f1_score\": [\"mean\", \"std\"],\n",
    "        }\n",
    "    )\n",
    "    metrics_df_all.columns = [\"_\".join(x) for x in metrics_df_all.columns]\n",
    "    metrics_df_all = metrics_df_all.reset_index()\n",
    "    metrics_df_all.to_csv(\n",
    "        os.path.join(BASE_DIR, f\"outputs/{sample_type}/{model_type}/metrics_pr.csv\"),\n",
    "        index=False,\n",
    "        float_format=\"%.3f\",\n",
    "    )\n",
    "    print(\"sample_type: \", sample_type)\n",
    "    print(metrics_df_all)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>precision_mean</th>\n",
       "      <th>precision_std</th>\n",
       "      <th>recall_mean</th>\n",
       "      <th>recall_std</th>\n",
       "      <th>f1_score_mean</th>\n",
       "      <th>f1_score_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Crypto</td>\n",
       "      <td>0.600674</td>\n",
       "      <td>0.121902</td>\n",
       "      <td>0.966356</td>\n",
       "      <td>0.017384</td>\n",
       "      <td>0.734561</td>\n",
       "      <td>0.088251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Giardia</td>\n",
       "      <td>0.905362</td>\n",
       "      <td>0.020524</td>\n",
       "      <td>0.984545</td>\n",
       "      <td>0.004398</td>\n",
       "      <td>0.943187</td>\n",
       "      <td>0.010864</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  category  precision_mean  precision_std  recall_mean  recall_std  \\\n",
       "0   Crypto        0.600674       0.121902     0.966356    0.017384   \n",
       "1  Giardia        0.905362       0.020524     0.984545    0.004398   \n",
       "\n",
       "   f1_score_mean  f1_score_std  \n",
       "0       0.734561      0.088251  \n",
       "1       0.943187      0.010864  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
