{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.ops import box_iou\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"/mnt/Enterprise/safal/AI_assisted_microscopy_system/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_annotation_file_fold_1 = os.path.join(\n",
    "    BASE_DIR,\n",
    "    \"cysts_dataset_all/brightfield_sample/fold_5/brightfield_sample_coco_annos_val.json\",\n",
    ")\n",
    "\n",
    "pred_annotation_file_fold_1 = os.path.join(\n",
    "    BASE_DIR,\n",
    "    \"outputs/brightfield_sample/faster_rcnn_x101_32x8d_fpn_mstrain_3x_coco/fold_5/results.bbox.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision_recall_f1(pred_annotations_file, gt_annotations_file):\n",
    "    # Precision x Recall is obtained individually by each class\n",
    "    # Loop through each class and calculate the precision and recall\n",
    "\n",
    "    # Precision = TP / (TP + FP)\n",
    "    # Recall = TP / (TP + FN)\n",
    "    gt_annotations = json.load(open(gt_annotations_file))\n",
    "    pred_annotations = json.load(open(pred_annotations_file))\n",
    "\n",
    "    gt_annotations_df = pd.DataFrame(gt_annotations[\"annotations\"])\n",
    "    pred_annotations_df = pd.DataFrame(pred_annotations)\n",
    "    \n",
    "    # change bbox width and height to x2, y2\n",
    "    pred_annotations_df[\"bbox\"] = pred_annotations_df[\"bbox\"].apply(\n",
    "        lambda x: [x[0], x[1], x[0] + x[2], x[1] + x[3]]\n",
    "    )\n",
    "    gt_annotations_df[\"bbox\"] = gt_annotations_df[\"bbox\"].apply(\n",
    "        lambda x: [x[0], x[1], x[0] + x[2], x[1] + x[3]]\n",
    "    )\n",
    "    \n",
    "    categories = sorted(gt_annotations_df.category_id.unique())\n",
    "\n",
    "    # dataframe to store the precision, recall and f1 score for each class\n",
    "    metrics_df = pd.DataFrame(\n",
    "        columns=[\"category_id\", \"precision\", \"recall\", \"f1_score\", \"TP\", \"FP\"]\n",
    "    )\n",
    "\n",
    "    for category in categories:\n",
    "        # get the ground truth annotations for the current class\n",
    "        gt_annotations_df_class = gt_annotations_df[\n",
    "            gt_annotations_df.category_id == category\n",
    "        ]\n",
    "        # get the predicted annotations for the current class\n",
    "        pred_annotations_df_class = pred_annotations_df[\n",
    "            pred_annotations_df.category_id == category\n",
    "        ]\n",
    "\n",
    "        # sort the predicted annotations by score\n",
    "        pred_annotations_df_class = pred_annotations_df_class.sort_values(\n",
    "            by=\"score\", ascending=False\n",
    "        )\n",
    "\n",
    "        # filter predictions with score > 0.3\n",
    "        pred_annotations_df_class = pred_annotations_df_class[\n",
    "            pred_annotations_df_class.score > 0.3\n",
    "        ]\n",
    "\n",
    "        true_positives_class = 0\n",
    "        false_positives_class = 0\n",
    "\n",
    "        # get image ids for the current class from both ground truth and predicted annotations\n",
    "        image_ids = pred_annotations_df_class[\"image_id\"].unique()\n",
    "        images_len = len(image_ids)\n",
    "\n",
    "        for image in image_ids:\n",
    "            # get the ground truth annotations for the current image\n",
    "            gt_annotations_df_image = gt_annotations_df_class[\n",
    "                gt_annotations_df_class.image_id == image\n",
    "            ]\n",
    "            # get the predicted annotations for the current image\n",
    "            pred_annotations_df_image = pred_annotations_df_class[\n",
    "                pred_annotations_df_class.image_id == image\n",
    "            ]\n",
    "\n",
    "            # get the ground truth bounding boxes\n",
    "            gt_bboxes = list(gt_annotations_df_image.bbox.values)\n",
    "            gt_bboxes = torch.tensor(gt_bboxes)\n",
    "\n",
    "            # get the predicted bounding boxes\n",
    "\n",
    "            # only take the predicted bounding boxes which have a score > 0.3\n",
    "            pred_bboxes = list(pred_annotations_df_image.bbox.values)\n",
    "            pred_bboxes = torch.tensor(pred_bboxes)\n",
    "\n",
    "\n",
    "            if len(gt_bboxes) == 0:\n",
    "                false_positives_class += len(pred_bboxes)\n",
    "                continue\n",
    "\n",
    "            # get the intersection over union for each predicted bounding box\n",
    "            ious = box_iou(gt_bboxes, pred_bboxes)\n",
    "\n",
    "            # get the maximum iou for each ground truth bounding box\n",
    "            max_ious, _ = torch.max(ious, dim=0)\n",
    "\n",
    "            # get the indices of the predicted bounding boxes with iou > 0.5\n",
    "            tp_indices = torch.where(max_ious >= 0.5)[0]\n",
    "            # print(ious)\n",
    "\n",
    "            # get the indices of the predicted bounding boxes with iou < 0.5\n",
    "            fp_indices = torch.where(max_ious < 0.5)[0]\n",
    "\n",
    "            # update the true positives and false positives\n",
    "            true_positives_class += len(tp_indices)\n",
    "            false_positives_class += len(fp_indices)\n",
    "\n",
    "        # print actual number of ground truth annotations and predicted annotations for the current class\n",
    "        print(\n",
    "            \"Actual:\",\n",
    "            gt_annotations_df_class.shape[0],\n",
    "            \"Predicted:\",\n",
    "            pred_annotations_df_class.shape[0],\n",
    "        )\n",
    "\n",
    "        # print true positives and false positives for the current class\n",
    "        print(\n",
    "            \"True positives:\",\n",
    "            true_positives_class,\n",
    "            \"False positives:\",\n",
    "            false_positives_class,\n",
    "        )\n",
    "        # calculate the precision and recall\n",
    "        precision = true_positives_class / (true_positives_class + false_positives_class)\n",
    "        recall = true_positives_class / gt_annotations_df_class.shape[0]\n",
    "\n",
    "        category_name = gt_annotations[\"categories\"][category][\"name\"]\n",
    "        print(f\"Category: {category_name}, Precision: {precision}, Recall: {recall}\")\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "        print(f\"F1 Score: {f1_score}\")\n",
    "\n",
    "        metrics_df = metrics_df.append(\n",
    "            {\n",
    "                \"category_id\": category,\n",
    "                \"precision\": precision,\n",
    "                \"recall\": recall,\n",
    "                \"f1_score\": f1_score,\n",
    "                \"TP\": true_positives_class,\n",
    "                \"FP\": false_positives_class,\n",
    "            },\n",
    "            ignore_index=True,\n",
    "        )\n",
    "\n",
    "    return metrics_df\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: 153 Predicted: 197\n",
      "True positives: 141 False positives: 56\n",
      "Category: Crypto, Precision: 0.7157360406091371, Recall: 0.9215686274509803\n",
      "F1 Score: 0.8057142857142857\n",
      "Actual: 68 Predicted: 88\n",
      "True positives: 62 False positives: 26\n",
      "Category: Giardia, Precision: 0.7045454545454546, Recall: 0.9117647058823529\n",
      "F1 Score: 0.794871794871795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3964489/789971966.py:121: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  metrics_df = metrics_df.append(\n",
      "/tmp/ipykernel_3964489/789971966.py:121: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  metrics_df = metrics_df.append(\n"
     ]
    }
   ],
   "source": [
    "metrics_df_fold_1 = calculate_precision_recall_f1(pred_annotation_file_fold_1, gt_annotation_file_fold_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category_id</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.715736</td>\n",
       "      <td>0.921569</td>\n",
       "      <td>0.805714</td>\n",
       "      <td>141.0</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.704545</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.794872</td>\n",
       "      <td>62.0</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category_id  precision    recall  f1_score     TP    FP\n",
       "0          0.0   0.715736  0.921569  0.805714  141.0  56.0\n",
       "1          1.0   0.704545  0.911765  0.794872   62.0  26.0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_df_fold_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_annotation_file_fold_3 = os.path.join(\n",
    "    BASE_DIR,\n",
    "    \"cysts_dataset_all/brightfield_sample/fold_4/brightfield_sample_coco_annos_val.json\",\n",
    ")\n",
    "\n",
    "pred_annotation_file_fold_3 = os.path.join(\n",
    "    BASE_DIR,\n",
    "    \"outputs/brightfield_sample/faster_rcnn_x101_32x8d_fpn_mstrain_3x_coco/fold_4/results.bbox.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: 146 Predicted: 190\n",
      "True positives: 128 False positives: 62\n",
      "Category: Crypto, Precision: 0.6736842105263158, Recall: 0.8767123287671232\n",
      "F1 Score: 0.7619047619047619\n",
      "Actual: 66 Predicted: 87\n",
      "True positives: 61 False positives: 26\n",
      "Category: Giardia, Precision: 0.7011494252873564, Recall: 0.9242424242424242\n",
      "F1 Score: 0.7973856209150327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3964489/789971966.py:121: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  metrics_df = metrics_df.append(\n",
      "/tmp/ipykernel_3964489/789971966.py:121: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  metrics_df = metrics_df.append(\n"
     ]
    }
   ],
   "source": [
    "metrics_df_fold_2 = calculate_precision_recall_f1(pred_annotation_file_fold_3, gt_annotation_file_fold_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category_id</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.673684</td>\n",
       "      <td>0.876712</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>128.0</td>\n",
       "      <td>62.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.701149</td>\n",
       "      <td>0.924242</td>\n",
       "      <td>0.797386</td>\n",
       "      <td>61.0</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category_id  precision    recall  f1_score     TP    FP\n",
       "0          0.0   0.673684  0.876712  0.761905  128.0  62.0\n",
       "1          1.0   0.701149  0.924242  0.797386   61.0  26.0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_df_fold_2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
